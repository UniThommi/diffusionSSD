# Diffusion Model Verification Guide

## √úbersicht

Dieses Dokument erkl√§rt die **5 Verifikationsmethoden** f√ºr dein NC‚ÜíOptical Signal Diffusionsmodell, basierend auf State-of-the-Art Diffusion Papers.

---

## Warum diese Methoden?

### ‚ö†Ô∏è Problem: Training mit nur 5 Epochen
Dein Training:
- **5 Epochen** √ó 50 Steps = **250 Trainingsschritte**
- Typisch f√ºr Diffusion: **100k-1M Steps**
- **Risiko:** Underfitting, Mode Collapse, Physics nicht gelernt

### ‚úì L√∂sung: Multi-Perspektiven Verifikation
Keine einzelne Metrik reicht! Wir pr√ºfen aus **5 komplement√§ren Perspektiven**.

---

## Die 5 Verifikationsmethoden

### 1Ô∏è‚É£ Reconstruction Fidelity (Ho et al. 2020)

**Paper:** [Denoising Diffusion Probabilistic Models (DDPM)](https://arxiv.org/abs/2006.11239)

**Was wird getestet:**
```
Input: NC-Parameter œÜ (Position, Energie, Richtung)
       ‚Üì
    Diffusion Model
       ‚Üì
Output: Optical Signal x‚ÇÄ
       ‚Üì
Vergleich mit Ground Truth
```

**Metriken:**
- **MSE (Mean Squared Error):** Wie genau ist Rekonstruktion?
- **Correlation:** Strukturelle √Ñhnlichkeit
- **F1-Score:** Precision/Recall f√ºr aktive Voxel

**Interpretation:**
- `MSE < 0.1`: Sehr gut
- `Correlation > 0.8`: Gut
- `F1 > 0.7`: Multiplizit√§t funktioniert

**Warum es verifiziert:**  
Zeigt, ob das Modell **√ºberhaupt etwas Sinnvolles** generiert.

---

### 2Ô∏è‚É£ Conditional Consistency (Rombach et al. 2022)

**Paper:** [Stable Diffusion](https://arxiv.org/abs/2112.10752)

**Was wird getestet:**
```
Fixiere œÜ ‚Üí Sample 50√ó ‚Üí Analysiere Konsistenz
```

**Idee:**  
- F√ºr **gleiche** NC-Parameter sollten Samples **√§hnlich** sein
- Aber **nicht identisch** (Stochastizit√§t ist ok)

**Metriken:**
- **Mean Consistency:** Durchschnitt aller Samples ‚âà Ground Truth?
- **Variance Plausibility:** Varianz physikalisch sinnvoll?

**Interpretation:**
```
Gut:     Mean(Samples) ‚âà Ground Truth, Var mittel
Schlecht: Samples kollabieren (Var‚Üí0) ODER explodieren (Var‚Üí‚àû)
```

**Warum es verifiziert:**  
Zeigt, ob Conditioning `œÜ ‚Üí x‚ÇÄ` **gelernt** wurde (nicht ignoriert!).

---

### 3Ô∏è‚É£ Interpolation Smoothness (Song et al. 2021)

**Paper:** [Score-Based Generative Modeling through SDEs](https://arxiv.org/abs/2011.13456)

**Was wird getestet:**
```
œÜ_A = [x=0, y=0, E=100keV]
        ‚Üì Interpoliere
œÜ_interp(Œ±) = (1-Œ±)¬∑œÜ_A + Œ±¬∑œÜ_B
        ‚Üì Interpoliere
œÜ_B = [x=100, y=100, E=200keV]

‚Üí Signal sollte smooth von A nach B wandern
```

**Metrik:**
- **Perceptual Path Length (PPL):** Summe der Abst√§nde zwischen Steps

**Interpretation:**
```
PPL ‚âà 1.0:  Perfekt (gerade Linie im Signal-Space)
PPL > 2.0:  Latenter Space ist nicht gut strukturiert
```

**Warum es verifiziert:**  
Zeigt, ob Modell **physikalische Kontinuit√§t** versteht (nicht nur Lookup-Table).

---

### 4Ô∏è‚É£ Physics Constraints (Karras et al. 2022)

**Paper:** [Elucidating the Design Space of Diffusion-Based Generative Models](https://arxiv.org/abs/2206.00364)

**Was wird getestet:**
1. **Energieerhaltung:** `Œ£(Signal) ‚àù E_gamma_tot_keV`
2. **R√§umliche Lokalit√§t:** Hits um NC-Position konzentriert
3. **Multiplizit√§tsbedingung:** ‚â•6 Voxel √ºber Threshold

**Interpretation:**
```
‚úì Gut:     Physik-Gesetze erf√ºllt
‚úó Schlecht: Energieverletzung, r√§umliche Artefakte
```

**Warum es verifiziert:**  
**Wichtigste Metrik!** Zeigt, ob Modell **echte Physik** lernt oder nur Patterns memoriert.

---

### 5Ô∏è‚É£ Classifier-Free Guidance (Ho & Salimans 2022)

**Paper:** [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598)

**Was wird getestet:**
```
Guidance-St√§rke w:
ŒµÃÉ = (1+w)¬∑Œµ_Œ∏(x_t, œÜ) - w¬∑Œµ_Œ∏(x_t, ‚àÖ)

w=0:   Keine Konditionierung (Baseline)
w=1:   Normale Konditionierung
w>1:   St√§rkere Konditionierung
```

**Interpretation:**
- H√∂heres `w` ‚Üí bessere Fidelity, weniger Diversit√§t
- Trade-off sollte existieren

**Warum es verifiziert:**  
Zeigt **Sensitivit√§t** des Modells auf Conditioning.

---

## Usage

### Quick Start
```bash
# Basis-Verifikation (alle Tests)
python verify_diffusion_model.py \
    --checkpoint ./checkpoints_cpu/checkpoint_epoch_5_model.weights.h5 \
    --n-test-events 100

# Mit Visualisierungen
python verify_diffusion_model.py \
    --checkpoint ./checkpoints_cpu/checkpoint_epoch_5_model.weights.h5 \
    --n-test-events 100 \
    --visualize \
    --show-trajectory
```

### Erwartete Laufzeit
- **Tests allein:** ~30-45 min (100 Events)
- **Mit Visualisierungen:** +10 min
- **Mit Trajektorie:** +5 min

### Output
```
verification_results/
‚îú‚îÄ‚îÄ verification_results.json           # Numerische Ergebnisse
‚îú‚îÄ‚îÄ verification_visualization.png      # Hauptvisualisierung
‚îî‚îÄ‚îÄ denoising_trajectory.png           # Trajektorien-Plot
```

---

## Interpretation der Ergebnisse

### Overall Score Interpretation

| Score | Bedeutung | Action |
|-------|-----------|--------|
| **‚â• 0.80** | ‚úì **Excellent** - Model ready for PMT selection | Proceed with confidence |
| **0.60-0.79** | ‚ö† **Good** - Minor issues, usable | Check which metric is low |
| **0.40-0.59** | ‚ö† **Fair** - Significant issues | More training recommended |
| **< 0.40** | ‚úó **Poor** - Model failed | Re-train with more epochs/data |

### Score-Komponenten Gewichte
```python
Overall = 0.3¬∑Fidelity + 0.2¬∑Consistency + 0.1¬∑Smoothness + 0.4¬∑Physics
```

**Warum Physics 40%?**  
‚Üí Physikalische Plausibilit√§t ist kritischer als perfekte Rekonstruktion!

---

## H√§ufige Probleme & L√∂sungen

### Problem 1: Low Fidelity Score (< 0.5)
**Symptom:** MSE hoch, Correlation niedrig

**Ursachen:**
- Zu wenig Training (5 Epochen!)
- Learning Rate zu hoch/niedrig
- Model zu klein (hidden_dim=512)

**L√∂sung:**
```bash
# Train l√§nger
python train_diffusion_cpu.py \
    --epochs 20 \
    --steps-per-epoch 500 \
    --learning-rate 5e-5
```

---

### Problem 2: Low Consistency Score (< 0.5)
**Symptom:** Samples f√ºr gleiches œÜ zu unterschiedlich

**Ursachen:**
- Conditioning nicht gelernt (Modell ignoriert œÜ)
- Zu viel Rauschen im Sampling

**L√∂sung:**
- Pr√ºfe Timestep-Embedding-Dimension (sollte ‚â•32 sein)
- Erh√∂he `n_sampling_steps` von 50 auf 100

---

### Problem 3: Low Physics Score (< 0.5)
**Symptom:** Energieverletzung, Multiplizit√§t falsch

**Ursachen:**
- Model lernt nur Patterns, nicht Physik
- Training Data zu klein/biased

**L√∂sung:**
- **Kritisch!** Mehr Daten sammeln
- Physics-Informed Loss hinzuf√ºgen:
```python
def physics_loss(x_gen, phi):
    energy_true = phi[:, 1]  # E_gamma_tot_keV
    energy_gen = tf.reduce_sum(x_gen, axis=1)
    
    energy_loss = tf.abs(energy_gen - energy_true)
    return energy_loss
```

---

### Problem 4: Model generiert nur Noise
**Symptom:** Alle Scores < 0.2

**Ursachen:**
- Training hat nicht funktioniert
- Checkpoint korrumpiert
- Falsches Target-Format

**L√∂sung:**
```bash
# Pr√ºfe Training-Loss
python train_diffusion_cpu.py --eval-only

# Falls Loss nicht sinkt ‚Üí Re-initialize
```

---

## Vergleich mit Baseline

### Random Baseline
Generiere zuf√§llige Signale ‚Üí Score sollte ~0.1 sein

### Memorization Check
Pr√ºfe, ob Modell nur Training-Daten memoriert:
```python
# Test auf UNSEEN Events (nicht im Training)
verifier = DiffusionVerifier(model, config)
verifier.run_all_tests()  # Should still score > 0.6
```

---

## Advanced: Custom Metrics

### Beispiel: Photon-Propagation Zeit
```python
def verify_photon_timing(x_gen, phi):
    """Pr√ºfe ob Photon-Ankunftszeiten physikalisch sind"""
    nc_position = phi[:3]  # x, y, z
    
    for voxel_idx, signal in enumerate(x_gen):
        if signal > threshold:
            voxel_pos = get_voxel_position(voxel_idx)
            distance = np.linalg.norm(voxel_pos - nc_position)
            
            # Speed of light in LAr: ~0.67c
            expected_time = distance / (0.67 * 3e8)
            
            # Check if timing matches...
```

---

## Literatur & Weiterf√ºhrendes

### Diffusion Models
1. **Ho et al. (2020)** - DDPM (Grundlage)  
   https://arxiv.org/abs/2006.11239

2. **Song et al. (2021)** - Score-Based (Theorie)  
   https://arxiv.org/abs/2011.13456

3. **Rombach et al. (2022)** - Stable Diffusion (Conditional)  
   https://arxiv.org/abs/2112.10752

4. **Karras et al. (2022)** - EDM (Best Practices)  
   https://arxiv.org/abs/2206.00364

### Physics-ML
5. **Cranmer et al. (2020)** - "The frontier of simulation-based inference"  
   PNAS, https://arxiv.org/abs/1911.01429

6. **Brehmer et al. (2020)** - "Mining for Dark Matter Substructure"  
   https://arxiv.org/abs/1909.02005

---

## FAQ

**Q: Warum DDIM statt DDPM Sampling?**  
A: DDIM ist deterministisch und **50√ó schneller** (50 steps statt 1000). Performance identisch.

**Q: Kann ich mit 5 Epochen √ºberhaupt gute Ergebnisse erwarten?**  
A: Nein. 5 Epochen sind ein **Proof-of-Concept**. F√ºr Production: ‚â•20 Epochen.

**Q: Was ist ein "guter" Overall Score f√ºr mein Use-Case?**  
A: F√ºr PMT-Selektion: **‚â•0.6** ausreichend (nur relative Importance z√§hlt). F√ºr exakte Rekonstruktion: **‚â•0.8** n√∂tig.

**Q: Soll ich Physics-Constraints in den Loss integrieren?**  
A: **Ja!** Weighted Sum:
```python
total_loss = mse_loss + 0.1 * energy_loss + 0.05 * multiplicity_loss
```

---

## Kontakt & Contributions

Bei Fragen oder Verbesserungsvorschl√§gen:
- Issue erstellen mit Verification-Log
- Score-Ergebnisse als JSON anh√§ngen
- Visualisierungen hilfreich

**Happy Verifying! üöÄ**